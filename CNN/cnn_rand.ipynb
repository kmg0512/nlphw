{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [3, 4, 5]\n",
    "feature = 100\n",
    "p = 0.5\n",
    "s = 3\n",
    "batch_size = 50\n",
    "k = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data... data loaded!\n",
      "number of sentences: 10662\n",
      "vocab size: 18764\n",
      "max sentence length: 56\n"
     ]
    }
   ],
   "source": [
    "# read MR dataset\n",
    "print(\"loading data...\", end=' ')\n",
    "files = [\"rt-polarity.neg\", \"rt-polarity.pos\"]\n",
    "revs = []\n",
    "vocab = defaultdict(float)\n",
    "max_l = 0\n",
    "for i in range(2):\n",
    "    with open(files[i], \"r\", encoding=\"cp1252\") as f:\n",
    "        for line in f:\n",
    "            orig_rev = clean_str(line.strip())\n",
    "            words = set(orig_rev.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "            datum  = {\"y\":i,\n",
    "                    \"text\": orig_rev}\n",
    "            if len(orig_rev.split()) > max_l:\n",
    "                max_l = len(orig_rev.split())\n",
    "            revs.append(datum)\n",
    "print(\"data loaded!\")\n",
    "\n",
    "print(\"number of sentences: \" + str(len(revs)))             # 10662\n",
    "print(\"vocab size: \" + str(len(vocab)))                     # 18764\n",
    "print(\"max sentence length: \" + str(max_l))                 # 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec vectors... word2vec loaded!\n",
      "num words already in word2vec: 16448\n"
     ]
    }
   ],
   "source": [
    "# read pre-trained word2vec\n",
    "print(\"loading word2vec vectors...\", end=' ')\n",
    "word_vecs = {}\n",
    "with open(\"GoogleNews-vectors-negative300.bin\", \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, _ = map(int, header.split())  # 3000000, 300\n",
    "    binary_len = 4 * k\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(\"latin1\")\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if word in vocab:\n",
    "            word_vecs[word] = torch.from_numpy(np.frombuffer(f.read(binary_len), dtype='float32'))\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "print(\"word2vec loaded!\")\n",
    "\n",
    "print(\"num words already in word2vec: \" + str(len(word_vecs)))    # 16448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "embedding = nn.Embedding(len(vocab)+1, k, padding_idx=0)\n",
    "W = {}                                                      # torch.Size([18765, 300])\n",
    "word_idx_map = {}\n",
    "W[\"rand\"] = W[\"vec\"] = embedding(torch.LongTensor(range(len(vocab)+1)))\n",
    "for word, i in zip(vocab, range(1,len(vocab)+1)):\n",
    "    if word in word_vecs:\n",
    "        W[\"vec\"][i] = word_vecs[word]\n",
    "    word_idx_map[word] = i\n",
    "print(\"dataset created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l, k=300):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l:\n",
    "        x.append(0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_static = [True, False, True]\n",
    "U = [\"rand\", \"vec\", \"vec\"]\n",
    "results = []\n",
    "train_x_idx, train_y, test_x_idx, test_y = [], [], [], []\n",
    "random.shuffle(revs)\n",
    "for rev, i in zip(revs, range(len(revs))):\n",
    "    sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k)\n",
    "    if i < len(revs) / 10:\n",
    "        test_x_idx.append(sent)\n",
    "        test_y.append(rev[\"y\"])\n",
    "    else:\n",
    "        train_x_idx.append(sent)\n",
    "        train_y.append(rev[\"y\"])\n",
    "train_x_idx = torch.tensor(train_x_idx)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_x_idx = torch.tensor(test_x_idx)\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9595 torch.Size([56, 300])\n",
      "torch.Size([9595, 1, 56, 300])\n",
      "torch.Size([9595, 1, 56, 300])\n",
      "torch.Size([9595, 1, 16800])\n"
     ]
    }
   ],
   "source": [
    "x = [W[\"rand\"][idx] for idx in [sent for sent in train_x_idx]] # 9595 * 64 * 300\n",
    "print(len(x), x[0].size())\n",
    "\n",
    "train_x = torch.Tensor(train_x_idx.size()[0], 1, max_l, k)\n",
    "print(train_x.size())\n",
    "\n",
    "for i in range(len(x)):\n",
    "    train_x[i][0] = x[i]\n",
    "print(train_x.size())\n",
    "\n",
    "train_x = train_x.reshape(train_x.size()[0], 1, max_l * k)\n",
    "print(train_x.size())\n",
    "\n",
    "x = [W[\"rand\"][idx] for idx in [sent for sent in test_x_idx]]\n",
    "test_x = torch.Tensor(test_x_idx.size()[0], 1, max_l, k)\n",
    "for i in range(len(x)):\n",
    "    test_x[i][0] = x[i]\n",
    "test_x = test_x.reshape(test_x.size()[0], 1, max_l * k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0287,  0.2812, -0.0518,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SelectBackward>), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "train = TensorDataset(train_x, train_y)\n",
    "print(train[0])\n",
    "train_loader = DataLoader(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hs, feature, k, p):\n",
    "        super(CNN, self).__init__()\n",
    "        for h in hs:\n",
    "            conv = nn.Conv1d(1, feature, h * k, stride=k)\n",
    "            setattr(self, 'conv%d' % h, conv)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(len(hs) * feature, 2)\n",
    "        self.loss = nn.LogSoftmax(dim=-1)\n",
    "        self.hs = hs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for h in self.hs:\n",
    "            conv = getattr(self, 'conv%d' % h)\n",
    "            out = self.drop(self.relu(conv(x)))\n",
    "            out = self.pool(out)\n",
    "            outs.append(out)\n",
    "        outs = torch.cat(outs, dim=1).reshape(-1, 300)\n",
    "        outs = self.fc(outs)\n",
    "        return self.loss(outs)\n",
    "    \n",
    "model = CNN(h, feature, k, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntorch.Size([1, 1, 16800])\\ntorch.Size([1, 100, 54])\\ntorch.Size([1, 100, 1])\\ntorch.Size([1, 100, 53])\\ntorch.Size([1, 100, 1])\\ntorch.Size([1, 100, 52])\\ntorch.Size([1, 100, 1])\\ntorch.Size([1, 300])\\ntorch.Size([1, 2])\\ntorch.Size([1, 2])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.forward(train_x[:1]).size())\n",
    "'''\n",
    "torch.Size([1, 1, 16800])\n",
    "torch.Size([1, 100, 54])\n",
    "torch.Size([1, 100, 1])\n",
    "torch.Size([1, 100, 53])\n",
    "torch.Size([1, 100, 1])\n",
    "torch.Size([1, 100, 52])\n",
    "torch.Size([1, 100, 1])\n",
    "torch.Size([1, 300])\n",
    "torch.Size([1, 2])\n",
    "torch.Size([1, 2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(115.9329)\n",
      "2 tensor(113.1467)\n",
      "3 tensor(109.7286)\n",
      "4 tensor(105.8845)\n",
      "5 tensor(103.2700)\n",
      "6 tensor(100.6148)\n",
      "7 tensor(97.4388)\n",
      "8 tensor(94.7471)\n",
      "9 tensor(92.1811)\n",
      "10 tensor(91.2009)\n"
     ]
    }
   ],
   "source": [
    "# 오차함수 객체\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화를 담당할 객체\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 시작\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    # 분할해 둔 데이터를 꺼내옴\n",
    "    for train_x, train_y in train_loader:\n",
    "        # 계산 그래프 구성\n",
    "        train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순전파 계산\n",
    "        output = model(train_x)\n",
    "        # 오차계산\n",
    "        loss = criterion(output, train_y)\n",
    "        # 역전파 계산\n",
    "        loss.backward()\n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        # 누적 오차 계산\n",
    "        total_loss += loss.data\n",
    "    print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7291471415182755"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계산 그래프 구성\n",
    "test_x, test_y = Variable(test_x), Variable(test_y)\n",
    "# 출력이 0 혹은 1이 되게 함\n",
    "result = torch.max(model(test_x).data, 1)[1]\n",
    "# 모형의 정확도 측정\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "\n",
    "# 모형의 정확도 출력\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
