{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read MR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data... data loaded!\n",
      "number of sentences: 10662\n",
      "vocab size: 18764\n",
      "max sentence length: 56\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data...\", end=' ')\n",
    "fnames = [\"rt-polarity.neg\", \"rt-polarity.pos\"]\n",
    "data = []\n",
    "vocab = defaultdict(int)\n",
    "max_l = 0\n",
    "for i in range(2):\n",
    "    with open(fnames[i], \"r\", encoding=\"cp1252\") as f:\n",
    "        for line in f:\n",
    "            sent = clean_str(line.strip())\n",
    "            words = set(sent.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "            datum  = {\"y\":i, \"text\": sent}\n",
    "            l = len(sent.split())\n",
    "            if l > max_l:\n",
    "                max_l = l\n",
    "            data.append(datum)\n",
    "print(\"data loaded!\")\n",
    "\n",
    "print(\"number of sentences: \" + str(len(data)))             # 10662\n",
    "print(\"vocab size: \" + str(len(vocab)))                     # 18764\n",
    "print(\"max sentence length: \" + str(max_l))                 # 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read pre-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec vectors... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14972d51fe2f4d0c8ed8a12b61503220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='load_bin_vec', max=3000000, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word2vec loaded!\n",
      "num words already in word2vec: 16448\n"
     ]
    }
   ],
   "source": [
    "print(\"loading word2vec vectors...\", end=' ')\n",
    "pre_trained = {}\n",
    "with open(\"GoogleNews-vectors-negative300.bin\", \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, k = map(int, header.split())  # 3000000, 300\n",
    "    binary_len = 4 * k\n",
    "    for line in tqdm_notebook(range(vocab_size), desc='load_bin_vec'):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(\"latin1\")\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if word in vocab:\n",
    "            pre_trained[word] = torch.from_numpy(np.frombuffer(f.read(binary_len), dtype='float32')).to(device)\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "print(\"word2vec loaded!\")\n",
    "\n",
    "print(\"num words already in word2vec: \" + str(len(pre_trained)))    # 16448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and oov word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "word_vecs = [torch.zeros(1, k).to(device)] # torch.Size([18765, 300])\n",
    "word_idx_map = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    if word in pre_trained:\n",
    "        word_vecs.append(pre_trained[word].reshape(1, k))\n",
    "    else:\n",
    "        word_vecs.append(torch.randn(1, k).to(device))\n",
    "    word_idx_map[word] = i + 1\n",
    "word_vecs = torch.cat(word_vecs)\n",
    "print(\"dataset created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=56, k=300):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    word_idx = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            word_idx.append(word_idx_map[word])\n",
    "    while len(word_idx) < max_l:\n",
    "        word_idx.append(0)\n",
    "    return word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = [\"non-static\", \"static\", \"non-static\", \"multichannel\"]\n",
    "h = [3, 4, 5]\n",
    "feature = 100\n",
    "p = 0.5\n",
    "s = 3\n",
    "x, y = [], []\n",
    "random.shuffle(data)\n",
    "for datum in data:\n",
    "    word_idx = get_idx_from_sent(datum[\"text\"], word_idx_map, max_l, k)\n",
    "    x.append(word_idx)\n",
    "    y.append(datum[\"y\"])\n",
    "split_idx = int(len(data) * 0.9) # 9595 1067 X 56\n",
    "train_x, train_y = torch.LongTensor(x[:split_idx]).to(device), torch.LongTensor(y[:split_idx]).to(device)\n",
    "test_x, test_y = torch.LongTensor(x[split_idx:]).to(device), torch.LongTensor(y[split_idx:]).to(device)\n",
    "train, test = TensorDataset(train_x, train_y), TensorDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, word_vecs, model_type, h, feature, k, p, max_l):\n",
    "        super(CNN, self).__init__()\n",
    "        v = word_vecs.size()[0]\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.ch = 1\n",
    "        self.emb = nn.Embedding(v, k, padding_idx=0)\n",
    "        if model_type != \"rand\":\n",
    "            self.emb.weight.data.copy_(word_vecs)\n",
    "            if model_type == \"static\":\n",
    "                self.emb.weight.requires_grad = False\n",
    "            elif model_type == \"multichannel\":\n",
    "                self.emb_multi = nn.Embedding(v, k, padding_idx=0)\n",
    "                self.emb_multi.weight.data.copy_(word_vecs)\n",
    "                self.emb_multi.weight.requires_grad = False\n",
    "                self.ch = 2\n",
    "\n",
    "        # Convolutional Layer\n",
    "        for w in h:\n",
    "            conv = nn.Conv1d(self.ch, feature, w * k, stride=k)\n",
    "            setattr(self, 'conv%d' % w, conv)\n",
    "\n",
    "        # Pooling Layer\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # FC Layer\n",
    "        self.fc = nn.Linear(len(h) * feature, 2)\n",
    "\n",
    "        # Other Layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.h = h\n",
    "        self.feature = feature\n",
    "        self.k = k\n",
    "        self.max_l = max_l\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        x = self.emb(input_x).reshape(-1, 1, self.max_l * self.k)\n",
    "        if self.ch == 2:\n",
    "            x_multi = self.emb_multi(input_x).reshape(-1, 1, self.max_l * self.k)\n",
    "            x = torch.cat((x, x_multi), dim=1)\n",
    "\n",
    "        outs = []\n",
    "        for w in self.h:\n",
    "            conv = getattr(self, 'conv%d' % w)\n",
    "            out = self.dropout(self.relu(conv(x)))\n",
    "            out = self.pool(out)\n",
    "            outs.append(out)\n",
    "        outs = torch.cat(outs, dim=1).reshape(-1, len(self.h) * self.feature)\n",
    "        output = self.fc(outs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f542b8acf6d84e309cb3bc324097f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='model_type', max=4, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe6c9a90bc0469e80af313f9dcadac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=1000, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfff13685fe405690e3f18ae7387694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in tqdm_notebook(range(4), desc='model_type', leave=False):\n",
    "    model = CNN(word_vecs, model_type, h, feature, k, p, max_l).to(device)\n",
    "    model.fc.weight.requires_grad = False\n",
    "    model.fc.bias.requires_grad = False\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.SGD(parameters, lr=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train model\n",
    "    for epoch in tqdm_notebook(range(1000), desc='epoch', leave=False):\n",
    "        total_loss = 0\n",
    "        for train_x, train_y in tqdm_notebook(train_loader, desc='train', leave=False):\n",
    "            train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = criterion(output, train_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(parameters, max_norm=s)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(epoch+1, total_loss)\n",
    "\n",
    "    # Test model\n",
    "    test_x, test_y = Variable(test_x), Variable(test_y)\n",
    "    result = torch.max(model(test_x).data, 1)[1]\n",
    "    accuracy = sum(test_y.cpu().data.numpy() == result.cpu().numpy()) / len(test_y.cpu().data.numpy())\n",
    "    accuracies.append(accuracy)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7872539831302718, 0.788191190253046, 0.795688847235239, 0.7975632614807873]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
