{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [3, 4, 5]\n",
    "feature = 100\n",
    "p = 0.5\n",
    "s = 3\n",
    "batch_size = 50\n",
    "k = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data... data loaded!\n",
      "number of sentences: 10662\n",
      "vocab size: 18764\n",
      "max sentence length: 56\n"
     ]
    }
   ],
   "source": [
    "# read MR dataset\n",
    "print(\"loading data...\", end=' ')\n",
    "files = [\"rt-polarity.neg\", \"rt-polarity.pos\"]\n",
    "revs = []\n",
    "vocab = defaultdict(float)\n",
    "max_l = 0\n",
    "for i in range(2):\n",
    "    with open(files[i], \"r\", encoding=\"cp1252\") as f:\n",
    "        for line in f:\n",
    "            orig_rev = clean_str(line.strip())\n",
    "            words = set(orig_rev.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "            datum  = {\"y\":i,\n",
    "                    \"text\": orig_rev,\n",
    "                    \"num_words\": len(orig_rev.split())}\n",
    "            if datum[\"num_words\"] > max_l:\n",
    "                max_l = datum[\"num_words\"]\n",
    "            revs.append(datum)\n",
    "print(\"data loaded!\")\n",
    "\n",
    "print(\"number of sentences: \" + str(len(revs)))             # 10662\n",
    "print(\"vocab size: \" + str(len(vocab)))                     # 18764\n",
    "print(\"max sentence length: \" + str(max_l))                 # 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec vectors... word2vec loaded!\n",
      "num words already in word2vec: 16448\n"
     ]
    }
   ],
   "source": [
    "# read pre-trained word2vec\n",
    "print(\"loading word2vec vectors...\", end=' ')\n",
    "word_vecs = {}\n",
    "with open(\"GoogleNews-vectors-negative300.bin\", \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, _ = map(int, header.split())  # 3000000, 300\n",
    "    binary_len = 4 * k\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(\"latin1\")\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if word in vocab:\n",
    "            word_vecs[word] = torch.from_numpy(np.frombuffer(f.read(binary_len), dtype='float32'))\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "print(\"word2vec loaded!\")\n",
    "\n",
    "print(\"num words already in word2vec: \" + str(len(word_vecs)))    # 16448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "embedding = nn.Embedding(len(vocab)+1, k, padding_idx=0)\n",
    "W = {}                                                      # torch.Size([18765, 300])\n",
    "word_idx_map = {}\n",
    "W[\"rand\"] = W[\"vec\"] = embedding(torch.LongTensor(range(len(vocab))))\n",
    "for word, i in zip(word_vecs, range(1,len(word_vecs)+1)):\n",
    "    W[\"vec\"][i] = word_vecs[word]\n",
    "    word_idx_map[word] = i\n",
    "print(\"dataset created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l, k=300):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l:\n",
    "        x.append(0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9595, 56]) torch.Size([9595])\n"
     ]
    }
   ],
   "source": [
    "non_static = [True, False, True]\n",
    "U = [\"rand\", \"vec\", \"vec\"]\n",
    "results = []\n",
    "train_x_idx, train_y, test_x_idx, test_y = [], [], [], []\n",
    "random.shuffle(revs)\n",
    "for rev, i in zip(revs, range(len(revs))):\n",
    "    sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k)\n",
    "    if i < len(revs) / 10:\n",
    "        test_x_idx.append(sent)\n",
    "        test_y.append(rev[\"y\"])\n",
    "    else:\n",
    "        train_x_idx.append(sent)\n",
    "        train_y.append(rev[\"y\"])\n",
    "train_x_idx = torch.tensor(train_x_idx)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_x_idx = torch.tensor(test_x_idx)\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9595 torch.Size([56, 300])\n"
     ]
    }
   ],
   "source": [
    "x = [W[\"rand\"][idx] for idx in [sent for sent in train_x_idx]] # 9595 * 64 * 300\n",
    "print(len(x), x[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9595, 1, 56, 300])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.Tensor(train_x_idx.size()[0], 1, max_l, k)\n",
    "print(train_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    train_x[i][0] = x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9595, 1, 16800])\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.reshape(train_x.size()[0], 1, max_l * k)\n",
    "print(train_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.1094,  0.1406, -0.0317,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SelectBackward>), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "train = TensorDataset(train_x, train_y)\n",
    "print(train[0])\n",
    "train_loader = DataLoader(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hs, feature, k, p):\n",
    "        super(CNN, self).__init__()\n",
    "        for h in hs:\n",
    "            conv = nn.Conv1d(1, feature, h * k)\n",
    "            setattr(self, 'conv%d' % h, conv)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(feature)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(len(hs) * feature, 2)\n",
    "        \n",
    "    def forward(self, x, hs):\n",
    "        outs = []\n",
    "        for h in hs:\n",
    "            conv = getattr(self, 'conv%d' % h)\n",
    "            out = self.dropout(nn.ReLu(conv(x)))\n",
    "            out = self.pool(out)\n",
    "            outs += out\n",
    "        outs = torch.cat(out, dim=-1)\n",
    "        outs = self.Linear(outs)\n",
    "        return nn.LogSoftmax(outs)\n",
    "    \n",
    "model = CNN(h, feature, k, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, total_param_norm, total_grad_norm = 0, 0, 0\n",
    "avg_loss, avg_param_norm, avg_grad_norm = 0, 0, 0\n",
    "sample_cnt = 0\n",
    "\n",
    "progress_bar = tqdm(train, \n",
    "                    desc='Training: ', \n",
    "                    unit='batch'\n",
    "                    ) if verbose is VERBOSE_BATCH_WISE else train\n",
    "# Iterate whole train-set.\n",
    "for idx, mini_batch in enumerate(progress_bar):\n",
    "    x, y = mini_batch.text, mini_batch.label\n",
    "    # Don't forget make grad zero before another back-prop.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = self.model(x)\n",
    "\n",
    "    loss = self.get_loss(y_hat, y)\n",
    "    loss.backward()\n",
    "\n",
    "    total_loss += loss\n",
    "    total_param_norm += utils.get_parameter_norm(self.model.parameters())\n",
    "    total_grad_norm += utils.get_grad_norm(self.model.parameters())\n",
    "\n",
    "    # Caluclation to show status\n",
    "    avg_loss = total_loss / (idx + 1)\n",
    "    avg_param_norm = total_param_norm / (idx + 1)\n",
    "    avg_grad_norm = total_grad_norm / (idx + 1)\n",
    "\n",
    "    if verbose is VERBOSE_BATCH_WISE:\n",
    "        progress_bar.set_postfix_str('|param|=%.2f |g_param|=%.2f loss=%.4e' % (avg_param_norm,\n",
    "                                                                                avg_grad_norm,\n",
    "                                                                                avg_loss\n",
    "                                                                                ))\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    sample_cnt += mini_batch.text.size(0)\n",
    "    if sample_cnt >= len(train.dataset.examples):\n",
    "        break\n",
    "\n",
    "if verbose is VERBOSE_BATCH_WISE:\n",
    "    progress_bar.close()\n",
    "\n",
    "return avg_loss, avg_param_norm, avg_grad_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
