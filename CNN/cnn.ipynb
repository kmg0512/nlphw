{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from time import sleep\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data():\n",
    "    \"\"\"\n",
    "    Loads data\n",
    "    \"\"\"\n",
    "    files = [\"rt-polarity.neg\", \"rt-polarity.pos\"]\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    max_l = 0\n",
    "    for i in range(2):\n",
    "        with open(files[i], \"r\", encoding=\"cp1252\") as f:\n",
    "            for line in f:\n",
    "                orig_rev = clean_str(line.strip())\n",
    "                words = set(orig_rev.split())\n",
    "                for word in words:\n",
    "                    vocab[word] += 1\n",
    "                datum  = {  \"y\":i,\n",
    "                            \"text\": orig_rev}\n",
    "                if len(orig_rev.split()) > max_l:\n",
    "                    max_l = len(orig_rev.split())\n",
    "                revs.append(datum)\n",
    "    return revs, vocab, max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())  # 3000000, 300\n",
    "        binary_len = 4 * layer1_size\n",
    "        for line in tqdm_notebook(range(vocab_size), desc='load_bin_vec'):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode(\"latin1\")\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            if word in vocab:\n",
    "                word_vecs[word] = torch.from_numpy(np.frombuffer(f.read(binary_len), dtype='float32'))\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs, layer1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l:\n",
    "        x.append(0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idx_data(revs, word_idx_map, max_l=56, k=300):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train_x_idx, train_y, test_x_idx, test_y = [], [], [], []\n",
    "    random.shuffle(revs)\n",
    "    for rev, i in zip(revs, range(len(revs))):\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k)\n",
    "        if i < len(revs) / 10:\n",
    "            test_x_idx.append(sent)\n",
    "            test_y.append(rev[\"y\"])\n",
    "        else:\n",
    "            train_x_idx.append(sent)\n",
    "            train_y.append(rev[\"y\"])\n",
    "    train_y = torch.LongTensor(train_y)\n",
    "    test_y = torch.LongTensor(test_y)\n",
    "    return train_x_idx, train_y, test_x_idx, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data... data loaded!\n",
      "number of sentences: 10662\n",
      "vocab size: 18764\n",
      "max sentence length: 56\n"
     ]
    }
   ],
   "source": [
    "# read MR dataset\n",
    "print(\"loading data...\", end=' ')\n",
    "revs, vocab, max_l = build_data()\n",
    "print(\"data loaded!\")\n",
    "\n",
    "print(\"number of sentences: \" + str(len(revs)))             # 10662\n",
    "print(\"vocab size: \" + str(len(vocab)))                     # 18764\n",
    "print(\"max sentence length: \" + str(max_l))                 # 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec vectors... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf51f37323524e1ca56b1ea059ed0e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='load_bin_vec', max=3000000, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word2vec loaded!\n",
      "num words already in word2vec: 16448\n"
     ]
    }
   ],
   "source": [
    "# read pre-trained word2vec\n",
    "print(\"loading word2vec vectors...\", end=' ')\n",
    "word_vecs, k = load_bin_vec(\"GoogleNews-vectors-negative300.bin\", vocab)\n",
    "print(\"word2vec loaded!\")\n",
    "\n",
    "print(\"num words already in word2vec: \" + str(len(word_vecs)))    # 16448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "embedding = nn.Embedding(len(vocab)+1, k, padding_idx=0)\n",
    "W = {}\n",
    "word_idx_map = {}\n",
    "W[\"rand\"] = W[\"vec\"] = embedding(torch.LongTensor(range(len(vocab)+1))) # torch.Size([18765, 300])\n",
    "for word, i in zip(vocab, range(1,len(vocab)+1)):\n",
    "    if word in word_vecs:\n",
    "        W[\"vec\"][i] = word_vecs[word]\n",
    "    word_idx_map[word] = i\n",
    "print(\"dataset created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(x_idx, W, max_l=56, k=300):\n",
    "    x = torch.Tensor(len(x_idx), 1, max_l * k)\n",
    "    for i, sent in enumerate(x_idx):\n",
    "        xx = []\n",
    "        for idx in sent:\n",
    "            xx.append(W[idx])\n",
    "        x[i] = torch.cat(tuple(xx))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hs, feature, k, p):\n",
    "        super(CNN, self).__init__()\n",
    "        for h in hs:\n",
    "            conv = nn.Conv1d(1, feature, h * k, stride=k)\n",
    "            setattr(self, 'conv%d' % h, conv)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(len(hs) * feature, 2)\n",
    "        self.loss = nn.LogSoftmax(dim=-1)\n",
    "        self.hs = hs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for h in self.hs:\n",
    "            conv = getattr(self, 'conv%d' % h)\n",
    "            out = self.drop(self.relu(conv(x)))\n",
    "            out = self.pool(out)\n",
    "            outs.append(out)\n",
    "        outs = torch.cat(outs, dim=1).reshape(-1, 300)\n",
    "        outs = self.fc(outs)\n",
    "        return self.loss(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_trainer(train_loader, W, non_static, h=[3,4,5], feature=100, p=0.5, s=3, batch_size=50, k=300):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = CNN(h, feature, k, p)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(1000), desc='epoch'):\n",
    "        total_loss = 0\n",
    "        for train_x, train_y in tqdm_notebook(train_loader, desc='train'):\n",
    "            train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = criterion(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc635763de8d4eca91680b062e894f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='i', max=3, style=ProgressStyle(description_width='initial')),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9190b6500144de8da93fac3dd98c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=1000, style=ProgressStyle(description_width='initâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa5aded366b4047958036a938a700f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c2afa72fa0472ab6fab10341274056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a816eb7551d46fb99138c660e786ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c11b2be47f4fd5bf9a8eda5821e307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9f6a69e2af4d1b847205478ecc3e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc0400f49f3443f8a094dc1f62dd532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb35be9f1abd4e13a231ea65d98f59dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26287a09b864b4587b1c476fd8f3eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce85fdb22064269b811f11811ba6db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8175fc1731e4a6d9e1de44a1cf2b31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(102.6312)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c457634ecfa4d33a0b10585356fa991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369911dcbeb448a5a300c634b567b700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d563d6811dd24df1a915fef6f9ce6f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3387fed21feb47ef811e8beaf2f1e380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542b76b32a2742a48eb260ff25ee982f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ee9a29ae2c4c3ca2d3fdebac2d79da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56ba3f9146b49e1a55bfc8be98a650e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbeb469e3914756a7bd1158fa5c350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a58d024574348108382386cea42c17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dded72ac1549609f1590facc46eb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(82.2685)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06040faace4246e186f66deb6e5cd9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3b949a1f640e9a024345a8a3fb5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9cecaca54c406298fa51d08ac3bfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c735b1ab011f4198b77585ec67ba4266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6aff2b5c4a4c6ca2ea9e84c257c965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=192, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_static = [True, False, True]\n",
    "U = [\"rand\", \"vec\", \"vec\"]\n",
    "accuracies = []\n",
    "train_x_idx, train_y, test_x_idx, test_y = make_idx_data(revs, word_idx_map, max_l=max_l, k=300)    # 9595 1067 X 56\n",
    "for i in tqdm_notebook(range(3), desc='i'):\n",
    "    train_x = make_data(train_x_idx, W[U[i]], max_l=max_l, k=300) # 9595, 1, 16800\n",
    "    train = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train, batch_size=50)\n",
    "    cnn_trainer(train_loader, W[U[i]], non_static[i], h=[3,4,5], feature=100, p=0.5, s=3, batch_size=50, k=300)\n",
    "    \n",
    "    test_x = make_data(test_x_idx, W[U[i]], max_l=max_l, k=300)\n",
    "    test_x, test_y = Variable(test_x), Variable(test_y)\n",
    "    result = torch.max(model(test_x).data, 1)[1]\n",
    "    accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
