{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import *\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from huffman import HuffmanCoding\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"SG\" # \"SG\" for skipgram, \"CBOW\" for CBOW\n",
    "part = \"part\" # \"part\" if you want to train on a part of corpus, \"full\" if you want to train on full corpus\n",
    "ns = 20 # 0 for hierarchical softmax, the other numbers would be the number of negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if part==\"part\":\n",
    "    text = open('text8',mode='r').readlines()[0][:1000000] #Load a part of corpus for debugging\n",
    "elif part==\"full\":\n",
    "    text = open('text8',mode='r').readlines()[0] #Load full corpus for submission\n",
    "else:\n",
    "    print(\"Unknown argument : \" + part)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = text.split()\n",
    "corpus=[]\n",
    "t = 10e-5\n",
    "f = Counter(word_seq)\n",
    "l = len(word_seq)\n",
    "\n",
    "for word in word_seq:\n",
    "    p = 1 - math.sqrt(t / (f[word] / l))\n",
    "    if p <= random():\n",
    "        corpus.append(word)\n",
    "\n",
    "stats = Counter(corpus)\n",
    "words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discard rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in corpus:\n",
    "    if stats[word]>4:\n",
    "        words.append(word)\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give an index number to a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {}\n",
    "w2i[\" \"]=0\n",
    "i = 1\n",
    "for word in vocab:\n",
    "    w2i[word] = i\n",
    "    i+=1\n",
    "i2w = {}\n",
    "for k,v in w2i.items():\n",
    "    i2w[v]=k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code dict for hierarchical softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdict={}\n",
    "freqdict[0]=10\n",
    "for word in vocab:\n",
    "    freqdict[w2i[word]]=stats[word]\n",
    "codes, nodes = HuffmanCoding().build(freqdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency table for negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqtable = [0,0,0]\n",
    "for k,v in stats.items():\n",
    "    f = int(v**0.75)\n",
    "    for _ in range(f):\n",
    "        if k in w2i.keys():\n",
    "            freqtable.append(w2i[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinput_set, ctarget_set = [], []\n",
    "sinput_set, starget_set = [], []\n",
    "window_size = 5\n",
    "\n",
    "for j in range(len(words)):\n",
    "    if j<window_size:\n",
    "        cinput_set.append([0 for _ in range(window_size-j)] + [w2i[words[k]] for k in range(j)] + [w2i[words[j+k+1]] for k in range(window_size)])\n",
    "        ctarget_set.append(w2i[words[j]])\n",
    "\n",
    "        sinput_set += [w2i[words[j]] for _ in range(window_size*2)]\n",
    "        starget_set += [0 for _ in range(window_size-j)] + [w2i[words[k]] for k in range(j)] + [w2i[words[j+k+1]] for k in range(window_size)]\n",
    "\n",
    "    elif j>=len(words)-window_size:\n",
    "        cinput_set.append([w2i[words[j-k-1]] for k in range(window_size)] + [w2i[words[len(words)-k-1]] for k in range(len(words)-j-1)] + [0 for _ in range(j+window_size-len(words)+1)])\n",
    "        ctarget_set.append(w2i[words[j]])\n",
    "\n",
    "        sinput_set += [w2i[words[j]] for _ in range(window_size*2)]\n",
    "        starget_set += [w2i[words[j-k-1]] for k in range(window_size)] + [w2i[words[len(words)-k-1]] for k in range(len(words)-j-1)] + [0 for _ in range(j+window_size-len(words)+1)]\n",
    "\n",
    "    else:\n",
    "        cinput_set.append([w2i[words[j-k-1]] for k in range(window_size)] + [w2i[words[j+k+1]] for k in range(window_size)])\n",
    "        ctarget_set.append(w2i[words[j]])\n",
    "\n",
    "        sinput_set += [w2i[words[j]] for _ in range(window_size*2)]\n",
    "        starget_set += [w2i[words[j-k-1]] for k in range(window_size)] + [w2i[words[j+k+1]] for k in range(window_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size\n",
      "3971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size\")\n",
    "print(len(w2i))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    return torch.sum(v1 * v2) / torch.sqrt(torch.sum(v1 * v1) * torch.sum(v2 * v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numwords = len(w2i)\n",
    "dimension = 64\n",
    "learning_rate = 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training samples\n",
      "60431\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c147cd7e8f4c5fbd7bd78661c12fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 10.174491, Time : 0.066962 sec\n",
      "Loss : 7.963453, Time : 1159.172956 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Xavier initialization of weight matrices\n",
    "W_in = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "W_out = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "losses=[]\n",
    "\n",
    "times = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"# of training samples\")\n",
    "print(len(cinput_set))\n",
    "print()\n",
    "\n",
    "#Training word2vec using SGD(Batch size : 1)\n",
    "for i, (contextWords, output) in tqdm_notebook(enumerate(zip(cinput_set,ctarget_set))):\n",
    "    #Only use the activated rows of the weight matrix\n",
    "    #activated should be torch.tensor(K,) so that activated W_out has the form of torch.tensor(K, D)\n",
    "    activated = [nodes[codes[output][:j]] for j in range(len(codes[output]))]\n",
    "    centerCode = codes[output]\n",
    "    \n",
    "    V, D = W_in.size()\n",
    "    K, _ = W_out[activated].size()\n",
    "\n",
    "    L = None\n",
    "    G_in = torch.zeros(1, D).to(device)\n",
    "    G_out = torch.ones(K, D).to(device)\n",
    "\n",
    "    inputVector = torch.sum(W_in[contextWords].t(), dim=1, keepdim=True)\n",
    "\n",
    "    p = 1\n",
    "    for j in range(K):\n",
    "        bti = 1 if centerCode[j] == '0' else -1\n",
    "        vj = W_out[activated][j].reshape(1, D)\n",
    "        vh = torch.mm(vj, inputVector)\n",
    "        p *= sigmoid(bti * vh)\n",
    "\n",
    "        tj = 1 if bti == 1 else 0\n",
    "        grad = sigmoid(vh) - tj\n",
    "\n",
    "        G_out[j] *= (grad * inputVector).reshape(D)\n",
    "        G_in += grad * vj\n",
    "\n",
    "    L = -torch.log(p).reshape(1)\n",
    "    \n",
    "    W_in[contextWords] -= learning_rate*G_in\n",
    "    W_out[activated] -= learning_rate*G_out\n",
    "\n",
    "    losses.append(L.item())\n",
    "    if i%50000==0:\n",
    "        avg_loss=sum(losses)/len(losses)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loss : %f, Time : %f sec\" %(avg_loss, elapsed_time,))\n",
    "        losses=[]\n",
    "        start_time = time.time()\n",
    "        times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization of weight matrices\n",
    "W_in = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "W_out = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "losses=[]\n",
    "\n",
    "times = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"# of training samples\")\n",
    "print(len(cinput_set))\n",
    "print()\n",
    "\n",
    "for i, (contextWords, output) in tqdm_notebook(enumerate(zip(cinput_set,ctarget_set))):\n",
    "    #Only use the activated rows of the weight matrix\n",
    "    #activated should be torch.tensor(K,) so that activated W_out has the form of torch.tensor(K, D)\n",
    "    activated = [output] + sample(stats, NS)\n",
    "    \n",
    "    V, D = W_in.size()\n",
    "    K, _ = W_out[activated].size()\n",
    "\n",
    "    L = None\n",
    "    G_in = None\n",
    "    G_out = torch.Tensor(K, D).to(device)\n",
    "\n",
    "    inputVector = torch.sum(W_in[contextWords].t(), dim=1, keepdim=True)\n",
    "\n",
    "    p = sigmoid(torch.mm(W_out[activated][0].reshape(1, D), inputVector))\n",
    "    L = -torch.log(p)\n",
    "    G_in = -(1 - p) * W_out[activated][0]\n",
    "    G_out[0] = -(1 - p) * inputVector.reshape(D)\n",
    "\n",
    "    for k in range(1, K):\n",
    "        q = sigmoid(-torch.mm(W_out[activated][k].reshape(1, D), inputVector))\n",
    "        loss -= torch.log(q)\n",
    "        G_in += (1 - q) * W_out[activated][k]\n",
    "        G_out[k] = (1 - q) * inputVector.reshape(D)\n",
    "        \n",
    "    W_in[inputs] -= learning_rate*G_in\n",
    "    W_out[activated] -= learning_rate*G_out\n",
    "\n",
    "    losses.append(L.item())\n",
    "    if i%50000==0:\n",
    "        avg_loss=sum(losses)/len(losses)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loss : %f, Time : %f sec\" %(avg_loss, elapsed_time,))\n",
    "        losses=[]\n",
    "        start_time = time.time()\n",
    "        times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skipgram HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization of weight matrices\n",
    "W_in = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "W_out = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "losses=[]\n",
    "\n",
    "times = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"# of training samples\")\n",
    "print(len(sinput_set))\n",
    "print()\n",
    "\n",
    "for i, (centerWord, output) in tqdm_notebook(enumerate(zip(sinput_set,starget_set))):\n",
    "    #Only use the activated rows of the weight matrix\n",
    "    #activated should be torch.tensor(K,) so that activated W_out has the form of torch.tensor(K, D)\n",
    "    activated = [nodes[codes[output][:j]] for j in range(len(codes[output]))]\n",
    "    \n",
    "    V, D = inputMatrix.size()\n",
    "    K, _ = outputMatrix.size()\n",
    "\n",
    "    L = None\n",
    "    G_in = torch.zeros(1, D).to(device)\n",
    "    G_out = torch.ones(K, D).to(device)\n",
    "\n",
    "\n",
    "    inputVector = W_in[centerWord].reshape(D, 1)\n",
    "\n",
    "    p = 1\n",
    "    for j in range(K):\n",
    "        bti = 1 if contextCode[j] == '0' else -1\n",
    "        vj = W_out[activated][j].reshape(1, D)\n",
    "        vh = torch.mm(vj, inputVector)\n",
    "        p *= sigmoid(bti * vh)\n",
    "\n",
    "        tj = 1 if bti == 1 else 0\n",
    "        grad = sigmoid(vh) - tj\n",
    "\n",
    "        G_out[j] = (grad * inputVector).reshape(D)\n",
    "        G_in += grad * vj\n",
    "\n",
    "    L = -torch.log(p).reshape(1)\n",
    "    \n",
    "    W_in[inputs] -= learning_rate*G_in.squeeze()\n",
    "    W_out[activated] -= learning_rate*G_out\n",
    "    \n",
    "    losses.append(L.item())\n",
    "    if i%50000==0:\n",
    "        avg_loss=sum(losses)/len(losses)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loss : %f, Time : %f sec\" %(avg_loss, elapsed_time,))\n",
    "        losses=[]\n",
    "        start_time = time.time()\n",
    "        times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skipgram NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization of weight matrices\n",
    "W_in = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "W_out = torch.randn(numwords, dimension).to(device) / (dimension**0.5)\n",
    "losses=[]\n",
    "\n",
    "times = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"# of training samples\")\n",
    "print(len(sinput_set))\n",
    "print()\n",
    "\n",
    "for i, (centerWord, output) in tqdm_notebook(enumerate(zip(sinput_set,target_set))):\n",
    "    #Only use the activated rows of the weight matrix\n",
    "    #activated should be torch.tensor(K,) so that activated W_out has the form of torch.tensor(K, D)\n",
    "    activated = [output] + sample(stats, NS)\n",
    "    L, G_in, G_out = skipgram_NS(inputs, W_in, W_out[activated])\n",
    "    V, D = inputMatrix.size()\n",
    "    K, _ = outputMatrix.size()\n",
    "\n",
    "    L = None\n",
    "    G_in = None\n",
    "    G_out = torch.Tensor(K, D).to(device)\n",
    "\n",
    "    inputVector = W_in[centerWord]\n",
    "\n",
    "    p = sigmoid(torch.mm(W_out[activated][0].reshape(1, D), inputVector.reshape(D, 1)))\n",
    "    L = -torch.log(p)\n",
    "    G_in = -(1 - p) * W_out[activated][0]\n",
    "    G_out[0] = -(1 - p) * inputVector\n",
    "\n",
    "    for k in range(1, K):\n",
    "        q = sigmoid(-torch.mm(W_out[activated][k].reshape(1, D), inputVector.reshape(D, 1)))\n",
    "        L -= torch.log(q)\n",
    "        G_in += (1 - q) * W_out[activated][k]\n",
    "        G_out[k] = (1 - q) * inputVector\n",
    "        \n",
    "    W_in[inputs] -= learning_rate*G_in.squeeze()\n",
    "    W_out[activated] -= learning_rate*G_out\n",
    "    \n",
    "    losses.append(L.item())\n",
    "    if i%50000==0:\n",
    "        avg_loss=sum(losses)/len(losses)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Loss : %f, Time : %f sec\" %(avg_loss, elapsed_time,))\n",
    "        losses=[]\n",
    "        start_time = time.time()\n",
    "        times.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogical_Reasoning_Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = open(\"questions-words.txt\", 'r').readlines()\n",
    "\n",
    "total = [0]\n",
    "answer = [0]\n",
    "cnt = -1\n",
    "for q in questions:\n",
    "    if q[0] == ':':\n",
    "        if cnt != -1:\n",
    "            print(\"Result:\", answer[cnt], '/', total[cnt])\n",
    "            print(\"Accuracy:\", round(answer[cnt] / total[cnt] * 100, 3), '%')\n",
    "            print()\n",
    "            total.append(0)\n",
    "            answer.append(0)\n",
    "\n",
    "        print(q[2:])\n",
    "        cnt += 1\n",
    "    else:\n",
    "        flag = False\n",
    "        for key in q.split():\n",
    "            if w2i.get(key) == None:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            total[cnt] += 1\n",
    "            continue\n",
    "\n",
    "        [x1, y1, x2, y2] = q.split()\n",
    "\n",
    "        vx1 = embedding[w2i[x1]]\n",
    "        vy1 = embedding[w2i[y1]]\n",
    "        vx2 = embedding[w2i[x2]]\n",
    "\n",
    "        vector = vx1 - vx2 + vy1\n",
    "\n",
    "        distance = [(cosine(vector, embedding[w]), w) for w in range(embedding.size()[0])]\n",
    "        closest = sorted(distance, key=lambda t: t[0], reverse=True)[:10]\n",
    "\n",
    "        if sum(map(lambda x: (x[1] == w2i[y2]), closest)) > 0:\n",
    "            answer[cnt] += 1\n",
    "        total[cnt] += 1\n",
    "\n",
    "print(\"Result:\", answer[cnt], '/', total[cnt])\n",
    "print(\"Accuracy:\", round(answer[cnt] / total[cnt] * 100, 3), '%')\n",
    "print()\n",
    "print(\"Total Result:\", sum(answer), '/', sum(total))\n",
    "print(\"Total Accuracy:\", round(sum(answer) / sum(total) * 100, 3), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
